# GitHub Crawler

An asynchronous web crawler for searching and analyzing GitHub repositories.

## Features

- Asynchronous repository search based on keywords
- Proxy support for request rate limiting bypass
- Language usage statistics extraction for repositories
- Results saved in JSON format

## Requirements

This script requires Python 3.7 or higher. All necessary dependencies are listed in the `requirements.txt` file.

## Installation

1. Clone the repository:
  git clone https://github.com/yourusername/github-crawler.git
  cd github-crawler
2. Create a virtual environment and activate it:
  python -m venv venv
  source venv/bin/activate  # On Windows use venv\Scripts\activate
3. Install the dependencies:
   pip install -r requirements.txt

## Usage

1. Run the script from the command line:
2. Enter search keywords and search type (Repositories or Issues) when prompted.

## Testing
1. To run unit tests, execute:
  python github_crawler.py test

## License
This project is distributed under the MIT License. See the LICENSE file for more information.

This README provides basic information about the project, installation and usage instructions, and a list of key features. You can customize it further to fit your specific needs or add more details if necessary.
